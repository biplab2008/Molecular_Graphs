{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccb04d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdmolops import GetAdjacencyMatrix\n",
    "from typing import Union, List, Type, Optional\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, GATConv\n",
    "from torch import nn, sigmoid\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import relu, dropout\n",
    "from torch.optim import Adam, RMSprop\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b482df03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 2]\n",
      "['C', 2]\n",
      "['C', 2]\n",
      "['C', 3]\n",
      "['C', 2]\n",
      "['C', 2]\n",
      "['O', 1]\n"
     ]
    }
   ],
   "source": [
    "lst_smiles=['C1=CC=C(C=C1)O','c1ccccc1']\n",
    "m=Chem.MolFromSmiles(lst_smiles[0])\n",
    "for atom in m.GetAtoms():\n",
    "    print([atom.GetSymbol(),atom.GetDegree()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d890507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(elem : str, allowable_elem : Union[List[str],List[int]])->List[bool]:\n",
    "    \n",
    "    if elem not in allowable_elem : elem = allowable_elem[-1]\n",
    "    \n",
    "    # [True if k==elem else False for k in allowable_elem]\n",
    "    # note map is faster\n",
    "    onehotvec=list(map(lambda dummy : dummy==elem,allowable_elem))\n",
    "    \n",
    "    return onehotvec\n",
    "\n",
    "def get_atom_features(atom : Type[Chem.rdchem.Mol],\n",
    "                  allowable_elem : List[str] =['C','N','O','S','F','Si','P','Cl','Br','Mg','Na',\n",
    "                                               'Ca','Fe','As','Al','I', 'B','V','K','Tl','Yb',\n",
    "                                               'Sb','Sn','Ag','Pd','Co','Se','Ti','Zn', 'Li','Ge',\n",
    "                                               'Cu','Au','Ni','Cd','In','Mn','Zr','Cr','Pt','Hg','Pb','Unknown'], \n",
    "                  use_H_explicit : bool =False,\n",
    "                  use_chirality : bool =False)->Optional[torch.Tensor]:\n",
    "    '''\n",
    "    function calcualtes node features\n",
    "    args: \n",
    "        - atom : Mol object\n",
    "        - allowable_elem : list of allowable elements\n",
    "    return :\n",
    "        - feats : node features\n",
    "    '''\n",
    "    # use H\n",
    "    if use_H_explicit : allowable_elem = ['H'] + allowable_elem\n",
    "    \n",
    "    # map elements \n",
    "    elem_list_enc=one_hot_encoding(atom.GetSymbol(),allowable_elem)\n",
    "    \n",
    "    # get atom degree\n",
    "    n_neighbors_enc = one_hot_encoding(atom.GetDegree(), [0, 1, 2, 3, 4])\n",
    "    \n",
    "    # formal charges\n",
    "    formal_charge_enc = one_hot_encoding(atom.GetFormalCharge(), [-3, -2, -1, 0, 1, 2, 3])\n",
    "    \n",
    "    # hybridization\n",
    "    hybridisation_type_enc = one_hot_encoding(str(atom.GetHybridization()), [\"S\", \"SP\", \"SP2\", \"SP3\", \"SP3D\", \"SP3D2\", \"OTHER\"])\n",
    "    \n",
    "    # atom belongs to a ring\n",
    "    is_in_a_ring_enc = [int(atom.IsInRing())]\n",
    "    \n",
    "    # presence of aromatic ring\n",
    "    is_aromatic_enc = [int(atom.GetIsAromatic())]\n",
    "    \n",
    "    # chirality and explicit H's\n",
    "    \n",
    "    # final encoding\n",
    "    f_enc=elem_list_enc + formal_charge_enc + hybridisation_type_enc + is_in_a_ring_enc + is_aromatic_enc\n",
    "    \n",
    "    return np.array(f_enc).astype('float')\n",
    "\n",
    "def get_bond_features(bond : Type[Chem.rdchem.Bond], use_stereochemistry : bool = True)->Optional[torch.Tensor]:\n",
    "    \n",
    "    # bond type only used common ones for organic molecules\n",
    "    bond_type_enc=one_hot_encoding(str(bond.GetBondType()),['SINGLE', 'DOUBLE', 'TRIPLE','AROMATIC'])\n",
    "    \n",
    "    # conjugation\n",
    "    bond_is_conj_enc = [int(bond.GetIsConjugated())]\n",
    "    \n",
    "    # is ring\n",
    "    bond_is_in_ring_enc = [int(bond.IsInRing())]\n",
    "    \n",
    "    bond_feature_enc = bond_type_enc + bond_is_conj_enc + bond_is_in_ring_enc\n",
    "    \n",
    "    if use_stereochemistry == True: \n",
    "        bond_feature_enc.extend(one_hot_encoding(str(bond.GetStereo()), [\"STEREOZ\", \"STEREOE\", \"STEREOANY\", \"STEREONONE\"]))\n",
    "    \n",
    "    return np.array(bond_feature_enc).astype('float')\n",
    "    \n",
    "def gen_graph_dset(smiles,labels):\n",
    "    # check num of nodses and edges\n",
    "    ethane='CC'\n",
    "    ethane=Chem.MolFromSmiles(ethane)\n",
    "    num_node_features=len(get_atom_features(ethane.GetAtoms()[0]))\n",
    "    num_edge_features=len(get_bond_features(ethane.GetBonds()[0]))\n",
    "\n",
    "    data_list=[]\n",
    "    print(len(labels))\n",
    "    for smile, label in zip(smiles,labels):\n",
    "        #print(smile)\n",
    "        #print(label)\n",
    "        mol=Chem.MolFromSmiles(smile)\n",
    "\n",
    "        num_atoms=mol.GetNumAtoms()\n",
    "        atom_features=np.zeros((num_atoms,num_node_features))\n",
    "\n",
    "        num_bonds=2*mol.GetNumBonds()\n",
    "        bond_features=np.zeros((num_bonds,num_edge_features))\n",
    "\n",
    "        # node features\n",
    "        for idx,atom in enumerate(mol.GetAtoms()):\n",
    "            atom_features[idx,:]=get_atom_features(atom)\n",
    "\n",
    "        # adjacency\n",
    "        adj=np.array([*np.nonzero(GetAdjacencyMatrix(mol))])\n",
    "\n",
    "        # bond features\n",
    "        bond_features=np.array([get_bond_features(mol.GetBondBetweenAtoms(int(i),int(j))) for (i,j) in zip(adj[0,:],adj[1,:])])\n",
    "        #for idx,(i,j) in enumerate(zip(adj[0,:],adj[1,:])):\n",
    "        #    bond_features[idx,:]=get_bond_features(mol.GetBondBetweenAtoms(int(i),int(j)))\n",
    "\n",
    "        # data\n",
    "        data_list.append(Data(x=torch.tensor(atom_features,dtype=torch.float),\n",
    "            edge_index=torch.tensor(adj,dtype=torch.long),\n",
    "            edge_attr=torch.tensor(bond_features,dtype=torch.float),\n",
    "            y=torch.tensor([label],dtype=torch.long)))\n",
    "        \n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56cc73e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C=C(Br)c1ccccc1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BrC(Br)Br</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BrC(Br)C(Br)Br</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BrC(Br)c1ccccc1OCC1CO1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BrC1CCC(Br)C(Br)CCC(Br)C(Br)CCC1Br</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               SMILES  Class\n",
       "0                     C=C(Br)c1ccccc1      0\n",
       "1                           BrC(Br)Br      0\n",
       "2                      BrC(Br)C(Br)Br      0\n",
       "3              BrC(Br)c1ccccc1OCC1CO1      0\n",
       "4  BrC1CCC(Br)C(Br)CCC(Br)C(Br)CCC1Br      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path=r'C:\\Users\\bdutta\\work\\pys\\AI_algos\\graphs\\Biodegradability_Prediction_QSAR_GCN-main\\data'\n",
    "df = pd.read_csv(os.path.join(path,'All-Public_dataset_Mordred.csv'), low_memory=False)\n",
    "df=df[['SMILES','Class']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef3ba0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2264,)\n",
      "(566,)\n",
      "2264\n",
      "566\n"
     ]
    }
   ],
   "source": [
    "# train & test loader\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['SMILES'].to_numpy(),df['Class'].to_numpy(), test_size=.2, shuffle=True, stratify=df['Class'], random_state=100)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "dset_train=gen_graph_dset(smiles=X_train,labels=y_train)\n",
    "dset_test=gen_graph_dset(smiles=X_test,labels=y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f57e9825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader for training\n",
    "train_loader = DataLoader(dataset = dset_train, batch_size = X_train.shape[0])\n",
    "test_loader = DataLoader(dataset = dset_test, batch_size = X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "424c0e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[35102, 59], edge_index=[2, 70968], edge_attr=[70968, 10], y=[2264], batch=[35102], ptr=[2265])\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for data in train_loader:\n",
    "    print(data)\n",
    "    if count==2 : break\n",
    "    count+=1\n",
    "d1=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e7f096e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0,  ..., 0, 0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1=next(iter(train_loader))\n",
    "d1.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5440cc80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c247e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self,n_f=[59,24,16,8],num_hops=3,num_classes=2):\n",
    "        \n",
    "        super(GCN,self).__init__()\n",
    "        lst=nn.ModuleList()\n",
    "        for k in range(num_hops):\n",
    "            lst.append(GCNConv(in_channels=n_f[k],out_channels=n_f[k+1]))\n",
    "            #lst.append(nn.ReLU())\n",
    "            \n",
    "        self.lin1=nn.Linear(n_f[-1],num_classes) \n",
    "        self.conv=lst\n",
    "    \n",
    "    def forward(self, x, edge_idx, batch):\n",
    "        # conv layers\n",
    "        for count, layer in enumerate(self.conv):\n",
    "            x = layer(x,edge_idx)\n",
    "            x = F.relu(x)\n",
    "            #if count == len(self.conv)-1 : x = F.relu(x)\n",
    "            \n",
    "        #readout\n",
    "        x = global_mean_pool(x,batch=batch)\n",
    "        \n",
    "        #linear layers\n",
    "        x = self.lin1(x)\n",
    "        \n",
    "        return x\n",
    "'''    \n",
    "def train(model,data_loader,criterion=nn.BCEWithLogitsLoss(),lr=1e-3):\n",
    "    model.train()\n",
    "    loss_lst=[]\n",
    "    optimizer=Adam(model.parameters(),lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        for data in data_loader:\n",
    "            # zeroing grads\n",
    "            optimizer.zero_grad()\n",
    "            # model out\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            loss = criterion(out,data.y.reshape(-1,1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('epoch: {}, loss : {}'.format(epoch,loss.item()))\n",
    "        loss_lst.append(loss.item())\n",
    "    \n",
    "    return loss_lst\n",
    "'''\n",
    "\n",
    "def train(model,data_loader,optimizer,criterion=nn.BCEWithLogitsLoss(),lr=1e-3):\n",
    "    model.train()\n",
    "    \n",
    "    for data in data_loader:\n",
    "        # zeroing grads\n",
    "        optimizer.zero_grad()\n",
    "        # model out\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        #loss = criterion(out,data.y.reshape(-1,1))\n",
    "        loss = criterion(out,data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def test(model,loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        out = model(data.x, data.edge_index, data.batch)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self,n_f=[59,16,8],heads=4,num_classes=2):\n",
    "        \n",
    "        super(GAT,self).__init__()\n",
    "        lst=nn.ModuleList()\n",
    "        num_hops=len(n_f)\n",
    "        for k in range(num_hops-1):\n",
    "            print(k)\n",
    "            if k ==0:\n",
    "                lst.append(GATConv(in_channels=n_f[0],out_channels=n_f[1],heads=heads))\n",
    "            else:\n",
    "                lst.append(GATConv(in_channels=n_f[k]*heads,out_channels=n_f[k+1],heads=heads))\n",
    "                \n",
    "            #lst.append(nn.ReLU())\n",
    "            \n",
    "        self.lin1=nn.Linear(n_f[-1]*heads,num_classes) \n",
    "        self.conv=lst\n",
    "    \n",
    "    def forward(self, x, edge_idx, edge_attr, batch):\n",
    "        # conv layers\n",
    "        for count, layer in enumerate(self.conv):\n",
    "            x = layer(x,edge_idx, edge_attr)\n",
    "            x = F.relu(x)\n",
    "            #if count == len(self.conv)-1 : x = F.relu(x)\n",
    "            \n",
    "        #readout\n",
    "        x = global_mean_pool(x,batch=batch)\n",
    "        \n",
    "        #linear layers\n",
    "        x = self.lin1(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "439d7521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GAT(\n",
       "  (lin1): Linear(in_features=32, out_features=2, bias=True)\n",
       "  (conv): ModuleList(\n",
       "    (0): GATConv(59, 16, heads=4)\n",
       "    (1): GATConv(64, 8, heads=4)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gat=GAT()\n",
    "gat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dac4450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_sparse import SparseTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16b687e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(d1.edge_index,torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8becfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for count, layer in enumerate(self.conv):\n",
    "    x = layer(d1.x,edge_idx, edge_attr)\n",
    "    x = F.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d01542e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m d2\u001b[38;5;241m=\u001b[39m\u001b[43mgat\u001b[49m\u001b[43m(\u001b[49m\u001b[43md1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43md1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43md1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m d2\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mrelu(d2)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#d2=gat(d1.x,d1.edge_index,d1.edge_attr)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyg\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'batch'"
     ]
    }
   ],
   "source": [
    "d2=gat(d1.x,d1.edge_index,d1.edge_attr)\n",
    "d2=F.relu(d2)\n",
    "#d2=gat(d1.x,d1.edge_index,d1.edge_attr)\n",
    "d2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb8649bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5577, 0.1485, 0.0000, 0.0000],\n",
       "        [0.5577, 0.1485, 0.0000, 0.0000],\n",
       "        [0.5577, 0.1485, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.6183, 0.0000, 0.0504, 0.0716],\n",
       "        [0.6247, 0.0253, 0.0173, 0.0000],\n",
       "        [0.6183, 0.0000, 0.0504, 0.0716]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cb13607",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3120, -0.2698],\n",
      "        [ 0.3220, -0.2585],\n",
      "        [ 0.3188, -0.2686],\n",
      "        ...,\n",
      "        [ 0.3259, -0.2605],\n",
      "        [ 0.3313, -0.2727],\n",
      "        [ 0.3289, -0.2701]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn=GCN(n_f=[59,24,16,8],num_hops=3)\n",
    "out=gcn(d1.x,d1.edge_index,d1.batch)\n",
    "print(out)\n",
    "d1.y.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ba093b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6130742049469965"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model=gcn,loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "14e9eeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, loss:  0.6690, Train Acc: 0.6122, Test Acc: 0.6131\n",
      "Epoch: 001, loss:  0.6599, Train Acc: 0.6122, Test Acc: 0.6131\n",
      "Epoch: 002, loss:  0.6518, Train Acc: 0.6122, Test Acc: 0.6131\n",
      "Epoch: 003, loss:  0.6427, Train Acc: 0.6122, Test Acc: 0.6131\n",
      "Epoch: 004, loss:  0.6329, Train Acc: 0.6122, Test Acc: 0.6131\n",
      "Epoch: 005, loss:  0.6220, Train Acc: 0.6458, Test Acc: 0.6555\n",
      "Epoch: 006, loss:  0.6092, Train Acc: 0.6860, Test Acc: 0.7014\n",
      "Epoch: 007, loss:  0.5959, Train Acc: 0.7297, Test Acc: 0.7261\n",
      "Epoch: 008, loss:  0.5830, Train Acc: 0.7306, Test Acc: 0.7367\n",
      "Epoch: 009, loss:  0.5714, Train Acc: 0.7292, Test Acc: 0.7314\n",
      "Epoch: 010, loss:  0.5622, Train Acc: 0.7310, Test Acc: 0.7385\n",
      "Epoch: 011, loss:  0.5550, Train Acc: 0.7372, Test Acc: 0.7438\n",
      "Epoch: 012, loss:  0.5498, Train Acc: 0.7443, Test Acc: 0.7491\n",
      "Epoch: 013, loss:  0.5447, Train Acc: 0.7504, Test Acc: 0.7509\n",
      "Epoch: 014, loss:  0.5378, Train Acc: 0.7549, Test Acc: 0.7544\n",
      "Epoch: 015, loss:  0.5294, Train Acc: 0.7588, Test Acc: 0.7562\n",
      "Epoch: 016, loss:  0.5206, Train Acc: 0.7628, Test Acc: 0.7650\n",
      "Epoch: 017, loss:  0.5125, Train Acc: 0.7646, Test Acc: 0.7650\n",
      "Epoch: 018, loss:  0.5066, Train Acc: 0.7619, Test Acc: 0.7703\n",
      "Epoch: 019, loss:  0.5032, Train Acc: 0.7668, Test Acc: 0.7739\n",
      "Epoch: 020, loss:  0.5005, Train Acc: 0.7646, Test Acc: 0.7739\n",
      "Epoch: 021, loss:  0.4974, Train Acc: 0.7672, Test Acc: 0.7721\n",
      "Epoch: 022, loss:  0.4952, Train Acc: 0.7659, Test Acc: 0.7633\n",
      "Epoch: 023, loss:  0.4970, Train Acc: 0.7655, Test Acc: 0.7668\n",
      "Epoch: 024, loss:  0.4999, Train Acc: 0.7646, Test Acc: 0.7668\n",
      "Epoch: 025, loss:  0.4931, Train Acc: 0.7668, Test Acc: 0.7633\n",
      "Epoch: 026, loss:  0.4955, Train Acc: 0.7672, Test Acc: 0.7721\n",
      "Epoch: 027, loss:  0.4912, Train Acc: 0.7672, Test Acc: 0.7721\n",
      "Epoch: 028, loss:  0.4895, Train Acc: 0.7641, Test Acc: 0.7633\n",
      "Epoch: 029, loss:  0.4879, Train Acc: 0.7677, Test Acc: 0.7633\n",
      "Epoch: 030, loss:  0.4854, Train Acc: 0.7725, Test Acc: 0.7703\n",
      "Epoch: 031, loss:  0.4852, Train Acc: 0.7721, Test Acc: 0.7668\n",
      "Epoch: 032, loss:  0.4826, Train Acc: 0.7668, Test Acc: 0.7633\n",
      "Epoch: 033, loss:  0.4825, Train Acc: 0.7650, Test Acc: 0.7633\n",
      "Epoch: 034, loss:  0.4801, Train Acc: 0.7650, Test Acc: 0.7703\n",
      "Epoch: 035, loss:  0.4802, Train Acc: 0.7646, Test Acc: 0.7668\n",
      "Epoch: 036, loss:  0.4785, Train Acc: 0.7703, Test Acc: 0.7703\n",
      "Epoch: 037, loss:  0.4788, Train Acc: 0.7637, Test Acc: 0.7686\n",
      "Epoch: 038, loss:  0.4776, Train Acc: 0.7659, Test Acc: 0.7774\n",
      "Epoch: 039, loss:  0.4776, Train Acc: 0.7677, Test Acc: 0.7774\n",
      "Epoch: 040, loss:  0.4765, Train Acc: 0.7659, Test Acc: 0.7686\n",
      "Epoch: 041, loss:  0.4762, Train Acc: 0.7672, Test Acc: 0.7721\n",
      "Epoch: 042, loss:  0.4752, Train Acc: 0.7708, Test Acc: 0.7756\n",
      "Epoch: 043, loss:  0.4749, Train Acc: 0.7725, Test Acc: 0.7756\n",
      "Epoch: 044, loss:  0.4739, Train Acc: 0.7703, Test Acc: 0.7721\n",
      "Epoch: 045, loss:  0.4737, Train Acc: 0.7730, Test Acc: 0.7721\n",
      "Epoch: 046, loss:  0.4727, Train Acc: 0.7739, Test Acc: 0.7668\n",
      "Epoch: 047, loss:  0.4725, Train Acc: 0.7743, Test Acc: 0.7721\n",
      "Epoch: 048, loss:  0.4714, Train Acc: 0.7756, Test Acc: 0.7756\n",
      "Epoch: 049, loss:  0.4712, Train Acc: 0.7743, Test Acc: 0.7739\n",
      "Epoch: 050, loss:  0.4702, Train Acc: 0.7778, Test Acc: 0.7686\n",
      "Epoch: 051, loss:  0.4700, Train Acc: 0.7752, Test Acc: 0.7739\n",
      "Epoch: 052, loss:  0.4692, Train Acc: 0.7783, Test Acc: 0.7721\n",
      "Epoch: 053, loss:  0.4686, Train Acc: 0.7765, Test Acc: 0.7686\n",
      "Epoch: 054, loss:  0.4681, Train Acc: 0.7769, Test Acc: 0.7686\n",
      "Epoch: 055, loss:  0.4673, Train Acc: 0.7805, Test Acc: 0.7739\n",
      "Epoch: 056, loss:  0.4670, Train Acc: 0.7787, Test Acc: 0.7686\n",
      "Epoch: 057, loss:  0.4662, Train Acc: 0.7787, Test Acc: 0.7686\n",
      "Epoch: 058, loss:  0.4657, Train Acc: 0.7800, Test Acc: 0.7739\n",
      "Epoch: 059, loss:  0.4652, Train Acc: 0.7778, Test Acc: 0.7703\n",
      "Epoch: 060, loss:  0.4644, Train Acc: 0.7774, Test Acc: 0.7756\n",
      "Epoch: 061, loss:  0.4639, Train Acc: 0.7796, Test Acc: 0.7739\n",
      "Epoch: 062, loss:  0.4634, Train Acc: 0.7792, Test Acc: 0.7756\n",
      "Epoch: 063, loss:  0.4626, Train Acc: 0.7800, Test Acc: 0.7721\n",
      "Epoch: 064, loss:  0.4619, Train Acc: 0.7800, Test Acc: 0.7703\n",
      "Epoch: 065, loss:  0.4614, Train Acc: 0.7800, Test Acc: 0.7686\n",
      "Epoch: 066, loss:  0.4607, Train Acc: 0.7822, Test Acc: 0.7686\n",
      "Epoch: 067, loss:  0.4599, Train Acc: 0.7858, Test Acc: 0.7686\n",
      "Epoch: 068, loss:  0.4593, Train Acc: 0.7809, Test Acc: 0.7668\n",
      "Epoch: 069, loss:  0.4587, Train Acc: 0.7858, Test Acc: 0.7703\n",
      "Epoch: 070, loss:  0.4581, Train Acc: 0.7849, Test Acc: 0.7668\n",
      "Epoch: 071, loss:  0.4573, Train Acc: 0.7871, Test Acc: 0.7668\n",
      "Epoch: 072, loss:  0.4565, Train Acc: 0.7871, Test Acc: 0.7668\n",
      "Epoch: 073, loss:  0.4558, Train Acc: 0.7862, Test Acc: 0.7686\n",
      "Epoch: 074, loss:  0.4550, Train Acc: 0.7862, Test Acc: 0.7686\n",
      "Epoch: 075, loss:  0.4543, Train Acc: 0.7884, Test Acc: 0.7668\n",
      "Epoch: 076, loss:  0.4536, Train Acc: 0.7902, Test Acc: 0.7703\n",
      "Epoch: 077, loss:  0.4531, Train Acc: 0.7867, Test Acc: 0.7686\n",
      "Epoch: 078, loss:  0.4531, Train Acc: 0.7880, Test Acc: 0.7756\n",
      "Epoch: 079, loss:  0.4542, Train Acc: 0.7911, Test Acc: 0.7774\n",
      "Epoch: 080, loss:  0.4560, Train Acc: 0.7893, Test Acc: 0.7774\n",
      "Epoch: 081, loss:  0.4550, Train Acc: 0.7889, Test Acc: 0.7686\n",
      "Epoch: 082, loss:  0.4503, Train Acc: 0.7906, Test Acc: 0.7721\n",
      "Epoch: 083, loss:  0.4484, Train Acc: 0.7933, Test Acc: 0.7774\n",
      "Epoch: 084, loss:  0.4506, Train Acc: 0.7902, Test Acc: 0.7756\n",
      "Epoch: 085, loss:  0.4505, Train Acc: 0.7920, Test Acc: 0.7756\n",
      "Epoch: 086, loss:  0.4468, Train Acc: 0.7937, Test Acc: 0.7774\n",
      "Epoch: 087, loss:  0.4463, Train Acc: 0.7928, Test Acc: 0.7774\n",
      "Epoch: 088, loss:  0.4480, Train Acc: 0.7933, Test Acc: 0.7774\n",
      "Epoch: 089, loss:  0.4463, Train Acc: 0.7951, Test Acc: 0.7721\n",
      "Epoch: 090, loss:  0.4440, Train Acc: 0.7964, Test Acc: 0.7827\n",
      "Epoch: 091, loss:  0.4447, Train Acc: 0.7933, Test Acc: 0.7756\n",
      "Epoch: 092, loss:  0.4451, Train Acc: 0.7977, Test Acc: 0.7792\n",
      "Epoch: 093, loss:  0.4432, Train Acc: 0.7955, Test Acc: 0.7721\n",
      "Epoch: 094, loss:  0.4420, Train Acc: 0.7937, Test Acc: 0.7756\n",
      "Epoch: 095, loss:  0.4427, Train Acc: 0.8008, Test Acc: 0.7845\n",
      "Epoch: 096, loss:  0.4425, Train Acc: 0.7959, Test Acc: 0.7739\n",
      "Epoch: 097, loss:  0.4409, Train Acc: 0.7951, Test Acc: 0.7739\n",
      "Epoch: 098, loss:  0.4401, Train Acc: 0.8008, Test Acc: 0.7792\n",
      "Epoch: 099, loss:  0.4405, Train Acc: 0.7933, Test Acc: 0.7756\n",
      "Epoch: 100, loss:  0.4404, Train Acc: 0.8017, Test Acc: 0.7809\n",
      "Epoch: 101, loss:  0.4393, Train Acc: 0.7955, Test Acc: 0.7739\n",
      "Epoch: 102, loss:  0.4382, Train Acc: 0.7951, Test Acc: 0.7739\n",
      "Epoch: 103, loss:  0.4379, Train Acc: 0.8026, Test Acc: 0.7827\n",
      "Epoch: 104, loss:  0.4381, Train Acc: 0.7942, Test Acc: 0.7809\n",
      "Epoch: 105, loss:  0.4379, Train Acc: 0.8021, Test Acc: 0.7827\n",
      "Epoch: 106, loss:  0.4370, Train Acc: 0.7955, Test Acc: 0.7739\n",
      "Epoch: 107, loss:  0.4360, Train Acc: 0.7986, Test Acc: 0.7756\n",
      "Epoch: 108, loss:  0.4353, Train Acc: 0.8017, Test Acc: 0.7827\n",
      "Epoch: 109, loss:  0.4352, Train Acc: 0.7955, Test Acc: 0.7774\n",
      "Epoch: 110, loss:  0.4353, Train Acc: 0.8030, Test Acc: 0.7827\n",
      "Epoch: 111, loss:  0.4357, Train Acc: 0.7973, Test Acc: 0.7774\n",
      "Epoch: 112, loss:  0.4360, Train Acc: 0.8061, Test Acc: 0.7809\n",
      "Epoch: 113, loss:  0.4362, Train Acc: 0.7951, Test Acc: 0.7792\n",
      "Epoch: 114, loss:  0.4355, Train Acc: 0.8061, Test Acc: 0.7845\n",
      "Epoch: 115, loss:  0.4340, Train Acc: 0.7973, Test Acc: 0.7739\n",
      "Epoch: 116, loss:  0.4324, Train Acc: 0.7981, Test Acc: 0.7739\n",
      "Epoch: 117, loss:  0.4314, Train Acc: 0.7999, Test Acc: 0.7809\n",
      "Epoch: 118, loss:  0.4313, Train Acc: 0.7986, Test Acc: 0.7756\n",
      "Epoch: 119, loss:  0.4317, Train Acc: 0.8026, Test Acc: 0.7862\n",
      "Epoch: 120, loss:  0.4321, Train Acc: 0.7968, Test Acc: 0.7792\n",
      "Epoch: 121, loss:  0.4322, Train Acc: 0.8034, Test Acc: 0.7845\n",
      "Epoch: 122, loss:  0.4319, Train Acc: 0.7977, Test Acc: 0.7792\n",
      "Epoch: 123, loss:  0.4313, Train Acc: 0.8048, Test Acc: 0.7845\n",
      "Epoch: 124, loss:  0.4304, Train Acc: 0.7968, Test Acc: 0.7774\n",
      "Epoch: 125, loss:  0.4294, Train Acc: 0.8034, Test Acc: 0.7845\n",
      "Epoch: 126, loss:  0.4284, Train Acc: 0.7968, Test Acc: 0.7756\n",
      "Epoch: 127, loss:  0.4278, Train Acc: 0.7981, Test Acc: 0.7756\n",
      "Epoch: 128, loss:  0.4273, Train Acc: 0.8061, Test Acc: 0.7792\n",
      "Epoch: 129, loss:  0.4271, Train Acc: 0.7999, Test Acc: 0.7756\n",
      "Epoch: 130, loss:  0.4272, Train Acc: 0.8039, Test Acc: 0.7898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 131, loss:  0.4278, Train Acc: 0.7951, Test Acc: 0.7792\n",
      "Epoch: 132, loss:  0.4295, Train Acc: 0.8057, Test Acc: 0.7756\n",
      "Epoch: 133, loss:  0.4333, Train Acc: 0.7959, Test Acc: 0.7686\n",
      "Epoch: 134, loss:  0.4379, Train Acc: 0.8034, Test Acc: 0.7827\n",
      "Epoch: 135, loss:  0.4360, Train Acc: 0.7977, Test Acc: 0.7792\n",
      "Epoch: 136, loss:  0.4273, Train Acc: 0.7999, Test Acc: 0.7792\n",
      "Epoch: 137, loss:  0.4250, Train Acc: 0.8083, Test Acc: 0.7809\n",
      "Epoch: 138, loss:  0.4307, Train Acc: 0.7964, Test Acc: 0.7792\n",
      "Epoch: 139, loss:  0.4306, Train Acc: 0.8052, Test Acc: 0.7915\n",
      "Epoch: 140, loss:  0.4241, Train Acc: 0.8043, Test Acc: 0.7862\n",
      "Epoch: 141, loss:  0.4254, Train Acc: 0.7968, Test Acc: 0.7809\n",
      "Epoch: 142, loss:  0.4285, Train Acc: 0.8052, Test Acc: 0.7898\n",
      "Epoch: 143, loss:  0.4242, Train Acc: 0.8061, Test Acc: 0.7862\n",
      "Epoch: 144, loss:  0.4228, Train Acc: 0.7995, Test Acc: 0.7774\n",
      "Epoch: 145, loss:  0.4258, Train Acc: 0.8057, Test Acc: 0.7862\n",
      "Epoch: 146, loss:  0.4239, Train Acc: 0.8026, Test Acc: 0.7845\n",
      "Epoch: 147, loss:  0.4215, Train Acc: 0.8004, Test Acc: 0.7774\n",
      "Epoch: 148, loss:  0.4233, Train Acc: 0.8061, Test Acc: 0.7880\n",
      "Epoch: 149, loss:  0.4231, Train Acc: 0.8030, Test Acc: 0.7845\n",
      "Epoch: 150, loss:  0.4209, Train Acc: 0.8034, Test Acc: 0.7792\n",
      "Epoch: 151, loss:  0.4210, Train Acc: 0.8061, Test Acc: 0.7880\n",
      "Epoch: 152, loss:  0.4220, Train Acc: 0.8021, Test Acc: 0.7792\n",
      "Epoch: 153, loss:  0.4207, Train Acc: 0.8052, Test Acc: 0.7862\n",
      "Epoch: 154, loss:  0.4195, Train Acc: 0.8052, Test Acc: 0.7845\n",
      "Epoch: 155, loss:  0.4201, Train Acc: 0.8012, Test Acc: 0.7774\n",
      "Epoch: 156, loss:  0.4204, Train Acc: 0.8070, Test Acc: 0.7845\n",
      "Epoch: 157, loss:  0.4191, Train Acc: 0.8070, Test Acc: 0.7862\n",
      "Epoch: 158, loss:  0.4183, Train Acc: 0.8043, Test Acc: 0.7809\n",
      "Epoch: 159, loss:  0.4189, Train Acc: 0.8061, Test Acc: 0.7862\n",
      "Epoch: 160, loss:  0.4191, Train Acc: 0.8034, Test Acc: 0.7827\n",
      "Epoch: 161, loss:  0.4180, Train Acc: 0.8065, Test Acc: 0.7898\n",
      "Epoch: 162, loss:  0.4172, Train Acc: 0.8096, Test Acc: 0.7915\n",
      "Epoch: 163, loss:  0.4173, Train Acc: 0.8052, Test Acc: 0.7827\n",
      "Epoch: 164, loss:  0.4176, Train Acc: 0.8070, Test Acc: 0.7898\n",
      "Epoch: 165, loss:  0.4173, Train Acc: 0.8061, Test Acc: 0.7862\n",
      "Epoch: 166, loss:  0.4165, Train Acc: 0.8096, Test Acc: 0.7933\n",
      "Epoch: 167, loss:  0.4158, Train Acc: 0.8101, Test Acc: 0.7933\n",
      "Epoch: 168, loss:  0.4155, Train Acc: 0.8034, Test Acc: 0.7827\n",
      "Epoch: 169, loss:  0.4155, Train Acc: 0.8061, Test Acc: 0.7880\n",
      "Epoch: 170, loss:  0.4158, Train Acc: 0.8057, Test Acc: 0.7845\n",
      "Epoch: 171, loss:  0.4159, Train Acc: 0.8092, Test Acc: 0.7880\n",
      "Epoch: 172, loss:  0.4157, Train Acc: 0.8070, Test Acc: 0.7862\n",
      "Epoch: 173, loss:  0.4153, Train Acc: 0.8083, Test Acc: 0.7898\n",
      "Epoch: 174, loss:  0.4145, Train Acc: 0.8070, Test Acc: 0.7898\n",
      "Epoch: 175, loss:  0.4137, Train Acc: 0.8083, Test Acc: 0.7933\n",
      "Epoch: 176, loss:  0.4132, Train Acc: 0.8118, Test Acc: 0.7968\n",
      "Epoch: 177, loss:  0.4132, Train Acc: 0.8092, Test Acc: 0.7880\n",
      "Epoch: 178, loss:  0.4134, Train Acc: 0.8110, Test Acc: 0.7915\n",
      "Epoch: 179, loss:  0.4145, Train Acc: 0.8017, Test Acc: 0.7809\n",
      "Epoch: 180, loss:  0.4166, Train Acc: 0.8083, Test Acc: 0.7880\n",
      "Epoch: 181, loss:  0.4194, Train Acc: 0.8021, Test Acc: 0.7756\n",
      "Epoch: 182, loss:  0.4222, Train Acc: 0.8096, Test Acc: 0.7880\n",
      "Epoch: 183, loss:  0.4196, Train Acc: 0.8057, Test Acc: 0.7774\n",
      "Epoch: 184, loss:  0.4141, Train Acc: 0.8105, Test Acc: 0.7933\n",
      "Epoch: 185, loss:  0.4112, Train Acc: 0.8136, Test Acc: 0.7933\n",
      "Epoch: 186, loss:  0.4141, Train Acc: 0.8048, Test Acc: 0.7809\n",
      "Epoch: 187, loss:  0.4168, Train Acc: 0.8127, Test Acc: 0.7880\n",
      "Epoch: 188, loss:  0.4139, Train Acc: 0.8110, Test Acc: 0.7898\n",
      "Epoch: 189, loss:  0.4104, Train Acc: 0.8105, Test Acc: 0.7862\n",
      "Epoch: 190, loss:  0.4107, Train Acc: 0.8123, Test Acc: 0.7880\n",
      "Epoch: 191, loss:  0.4132, Train Acc: 0.8043, Test Acc: 0.7845\n",
      "Epoch: 192, loss:  0.4142, Train Acc: 0.8140, Test Acc: 0.7915\n",
      "Epoch: 193, loss:  0.4119, Train Acc: 0.8114, Test Acc: 0.7862\n",
      "Epoch: 194, loss:  0.4094, Train Acc: 0.8123, Test Acc: 0.7898\n",
      "Epoch: 195, loss:  0.4086, Train Acc: 0.8136, Test Acc: 0.7968\n",
      "Epoch: 196, loss:  0.4096, Train Acc: 0.8034, Test Acc: 0.7827\n",
      "Epoch: 197, loss:  0.4110, Train Acc: 0.8136, Test Acc: 0.7915\n",
      "Epoch: 198, loss:  0.4106, Train Acc: 0.8061, Test Acc: 0.7827\n",
      "Epoch: 199, loss:  0.4094, Train Acc: 0.8171, Test Acc: 0.7933\n",
      "Epoch: 200, loss:  0.4076, Train Acc: 0.8149, Test Acc: 0.7968\n",
      "Epoch: 201, loss:  0.4072, Train Acc: 0.8101, Test Acc: 0.7845\n",
      "Epoch: 202, loss:  0.4080, Train Acc: 0.8136, Test Acc: 0.7898\n",
      "Epoch: 203, loss:  0.4091, Train Acc: 0.8052, Test Acc: 0.7862\n",
      "Epoch: 204, loss:  0.4098, Train Acc: 0.8149, Test Acc: 0.7951\n",
      "Epoch: 205, loss:  0.4092, Train Acc: 0.8065, Test Acc: 0.7845\n",
      "Epoch: 206, loss:  0.4081, Train Acc: 0.8185, Test Acc: 0.7915\n",
      "Epoch: 207, loss:  0.4064, Train Acc: 0.8145, Test Acc: 0.7933\n",
      "Epoch: 208, loss:  0.4053, Train Acc: 0.8136, Test Acc: 0.7915\n",
      "Epoch: 209, loss:  0.4054, Train Acc: 0.8163, Test Acc: 0.7933\n",
      "Epoch: 210, loss:  0.4063, Train Acc: 0.8057, Test Acc: 0.7809\n",
      "Epoch: 211, loss:  0.4081, Train Acc: 0.8154, Test Acc: 0.7880\n",
      "Epoch: 212, loss:  0.4091, Train Acc: 0.8043, Test Acc: 0.7827\n",
      "Epoch: 213, loss:  0.4100, Train Acc: 0.8140, Test Acc: 0.7915\n",
      "Epoch: 214, loss:  0.4079, Train Acc: 0.8127, Test Acc: 0.7898\n",
      "Epoch: 215, loss:  0.4048, Train Acc: 0.8136, Test Acc: 0.7915\n",
      "Epoch: 216, loss:  0.4035, Train Acc: 0.8185, Test Acc: 0.7933\n",
      "Epoch: 217, loss:  0.4052, Train Acc: 0.8070, Test Acc: 0.7845\n",
      "Epoch: 218, loss:  0.4091, Train Acc: 0.8171, Test Acc: 0.7880\n",
      "Epoch: 219, loss:  0.4117, Train Acc: 0.8048, Test Acc: 0.7827\n",
      "Epoch: 220, loss:  0.4120, Train Acc: 0.8127, Test Acc: 0.7951\n",
      "Epoch: 221, loss:  0.4063, Train Acc: 0.8132, Test Acc: 0.7845\n",
      "Epoch: 222, loss:  0.4025, Train Acc: 0.8096, Test Acc: 0.7862\n",
      "Epoch: 223, loss:  0.4042, Train Acc: 0.8158, Test Acc: 0.7933\n",
      "Epoch: 224, loss:  0.4065, Train Acc: 0.8092, Test Acc: 0.7862\n",
      "Epoch: 225, loss:  0.4050, Train Acc: 0.8189, Test Acc: 0.7915\n",
      "Epoch: 226, loss:  0.4022, Train Acc: 0.8163, Test Acc: 0.7968\n",
      "Epoch: 227, loss:  0.4014, Train Acc: 0.8096, Test Acc: 0.7862\n",
      "Epoch: 228, loss:  0.4025, Train Acc: 0.8180, Test Acc: 0.7915\n",
      "Epoch: 229, loss:  0.4036, Train Acc: 0.8110, Test Acc: 0.7880\n",
      "Epoch: 230, loss:  0.4033, Train Acc: 0.8180, Test Acc: 0.7968\n",
      "Epoch: 231, loss:  0.4014, Train Acc: 0.8163, Test Acc: 0.7986\n",
      "Epoch: 232, loss:  0.4003, Train Acc: 0.8132, Test Acc: 0.7898\n",
      "Epoch: 233, loss:  0.4011, Train Acc: 0.8189, Test Acc: 0.7951\n",
      "Epoch: 234, loss:  0.4028, Train Acc: 0.8101, Test Acc: 0.7880\n",
      "Epoch: 235, loss:  0.4039, Train Acc: 0.8158, Test Acc: 0.7933\n",
      "Epoch: 236, loss:  0.4032, Train Acc: 0.8127, Test Acc: 0.7845\n",
      "Epoch: 237, loss:  0.4012, Train Acc: 0.8167, Test Acc: 0.7933\n",
      "Epoch: 238, loss:  0.3994, Train Acc: 0.8154, Test Acc: 0.7968\n",
      "Epoch: 239, loss:  0.3991, Train Acc: 0.8127, Test Acc: 0.7862\n",
      "Epoch: 240, loss:  0.3998, Train Acc: 0.8198, Test Acc: 0.8004\n",
      "Epoch: 241, loss:  0.4006, Train Acc: 0.8123, Test Acc: 0.7862\n",
      "Epoch: 242, loss:  0.4015, Train Acc: 0.8171, Test Acc: 0.7951\n",
      "Epoch: 243, loss:  0.4018, Train Acc: 0.8127, Test Acc: 0.7845\n",
      "Epoch: 244, loss:  0.4011, Train Acc: 0.8193, Test Acc: 0.7968\n",
      "Epoch: 245, loss:  0.3991, Train Acc: 0.8136, Test Acc: 0.7933\n",
      "Epoch: 246, loss:  0.3978, Train Acc: 0.8158, Test Acc: 0.7915\n",
      "Epoch: 247, loss:  0.3976, Train Acc: 0.8171, Test Acc: 0.7986\n",
      "Epoch: 248, loss:  0.3982, Train Acc: 0.8114, Test Acc: 0.7880\n",
      "Epoch: 249, loss:  0.3999, Train Acc: 0.8189, Test Acc: 0.7915\n"
     ]
    }
   ],
   "source": [
    "optimizer=Adam(gcn.parameters(),lr=1e-2)\n",
    "epochs=250\n",
    "loss_list=[]\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    loss = train(model=gcn,\n",
    "                 data_loader=train_loader,\n",
    "                 optimizer=optimizer,\n",
    "                 criterion=nn.CrossEntropyLoss())\n",
    "    \n",
    "    train_acc = test(model=gcn,loader=train_loader)\n",
    "    test_acc = test(model=gcn,loader=test_loader)\n",
    "    \n",
    "    loss_list.append(loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch:03d}, loss: {loss : .4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "af5797ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     1,     1,  ..., 35100, 35100, 35101],\n",
       "        [    1,     0,     2,  ..., 35098, 35101, 35100]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e25d963d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35102, 8])\n",
      "torch.Size([2264, 8])\n"
     ]
    }
   ],
   "source": [
    "x=d1.x\n",
    "for layer in gcn.conv:\n",
    "    x=layer(x,d1.edge_index)\n",
    "y=global_mean_pool(x,d1.batch)    \n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3a9f8114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2677, -0.4080,  0.0694,  ..., -0.5944,  0.1971, -0.2537],\n",
       "        [ 0.0874, -0.3078,  0.0867,  ..., -0.6759,  0.3521, -0.2275],\n",
       "        [ 0.2329, -0.4535,  0.0303,  ..., -0.4110,  0.1651, -0.2481],\n",
       "        ...,\n",
       "        [ 0.0410, -0.3021,  0.0523,  ..., -0.6461,  0.3569, -0.2481],\n",
       "        [ 0.1258, -0.2006,  0.1695,  ..., -0.7600,  0.3040, -0.0679],\n",
       "        [ 0.0774, -0.2730,  0.1426,  ..., -0.5744,  0.1906, -0.2397]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8b7db2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e03173ef",
   "metadata": {},
   "source": [
    "# Jibbersih/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e8e9743b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7311)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sigmoid(torch.tensor(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b4cd2e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[21, 59], edge_index=[2, 44], edge_attr=[44, 10], y=[1])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1=dset_train[1]\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2db33d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn1=GCNConv(in_channels=59,out_channels=10)\n",
    "gcn2=GCNConv(in_channels=10,out_channels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "134c2cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[30, 59], edge_index=[2, 60], edge_attr=[60, 10], y=[2], batch=[30], ptr=[3])\n",
      "DataBatch(x=[29, 59], edge_index=[2, 60], edge_attr=[60, 10], y=[2], batch=[29], ptr=[3])\n",
      "DataBatch(x=[47, 59], edge_index=[2, 100], edge_attr=[100, 10], y=[2], batch=[47], ptr=[3])\n",
      "DataBatch(x=[20, 59], edge_index=[2, 42], edge_attr=[42, 10], y=[2], batch=[20], ptr=[3])\n",
      "DataBatch(x=[28, 59], edge_index=[2, 56], edge_attr=[56, 10], y=[2], batch=[28], ptr=[3])\n",
      "DataBatch(x=[23, 59], edge_index=[2, 48], edge_attr=[48, 10], y=[2], batch=[23], ptr=[3])\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for data in train_loader:\n",
    "    print(data)\n",
    "    if count==5 : break\n",
    "    count+=1\n",
    "d1=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "406dc46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): GCNConv(59, 10)\n",
       "  (1): GCNConv(10, 5)\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn1=nn.Sequential(GCNConv(in_channels=59,out_channels=10),GCNConv(in_channels=10,out_channels=5))\n",
    "gcn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "de59f969",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [107], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgcn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyg\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'x'"
     ]
    }
   ],
   "source": [
    "gcn1(x=d1.x,edge_index=d1.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6f51a407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([29, 10])\n",
      "torch.Size([29, 5])\n",
      "tensor([[-0.3221,  0.2483, -0.1072, -0.3957,  0.1450],\n",
      "        [-1.0307,  0.2611, -0.4610, -0.5807, -0.4320]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x=gcn1(x=d1.x,edge_index=d1.edge_index)\n",
    "print(x.shape)\n",
    "x=gcn2(x=x,edge_index=d1.edge_index)\n",
    "print(x.shape)\n",
    "\n",
    "y=global_mean_pool(x,batch=d1.batch)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3a32acca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.4278, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a3b389ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_smiles=['C=C(Br)c1ccccc1']#,'c1ccccc1']\n",
    "lst_labels=[1,1]\n",
    "# check num of nodses and edges\n",
    "ethane='CC'\n",
    "ethane=Chem.MolFromSmiles(ethane)\n",
    "num_node_features=len(get_atom_features(ethane.GetAtoms()[0]))\n",
    "num_edge_features=len(get_bond_features(ethane.GetBonds()[0]))\n",
    "\n",
    "data_list=[]\n",
    "for smile, label in zip(lst_smiles,lst_labels):\n",
    "    #print(smile)\n",
    "    #print(label)\n",
    "    mol=Chem.MolFromSmiles(smile)\n",
    "\n",
    "    num_atoms=mol.GetNumAtoms()\n",
    "    atom_features=np.zeros((num_atoms,num_node_features))\n",
    "\n",
    "    num_bonds=2*mol.GetNumBonds()\n",
    "    bond_features=np.zeros((num_bonds,num_edge_features))\n",
    "\n",
    "    # node features\n",
    "    for atom in mol.GetAtoms():\n",
    "        atom_features[atom.GetIdx(),:]=get_atom_features(atom)\n",
    "\n",
    "    # adjacency\n",
    "    adj=np.array([*np.nonzero(GetAdjacencyMatrix(mol))])\n",
    "\n",
    "    # bond features\n",
    "    for idx,(i,j) in enumerate(zip(adj[0,:],adj[1,:])):\n",
    "        bond_features[idx,:]=get_bond_features(mol.GetBondBetweenAtoms(int(i),int(j)))\n",
    "        \n",
    "    bond_features=np.array([get_bond_features(mol.GetBondBetweenAtoms(int(i),int(j))) for (i,j) in zip(adj[0,:],adj[1,:])])\n",
    "\n",
    "    # data\n",
    "    data_list+=[Data(x=torch.tensor(atom_features,dtype=torch.float),\n",
    "        edge_index=torch.tensor(adj,dtype=torch.long),\n",
    "        edge_attr=torch.tensor(bond_features,dtype=torch.float),y=torch.LongTensor([label]))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "44438aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "lst_smiles=['C1=CC=C(C=C1)O']\n",
    "lst_labels=[1]\n",
    "\n",
    "for (smiles, y_val) in zip(lst_smiles, lst_labels):\n",
    "\n",
    "    # convert SMILES to RDKit mol object\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    # get feature dimensions\n",
    "    n_nodes = mol.GetNumAtoms()\n",
    "    n_edges = 2*mol.GetNumBonds()\n",
    "    unrelated_smiles = \"O=O\"\n",
    "    unrelated_mol = Chem.MolFromSmiles(unrelated_smiles)\n",
    "    n_node_features = len(get_atom_features(unrelated_mol.GetAtomWithIdx(0)))\n",
    "    n_edge_features = len(get_bond_features_u(unrelated_mol.GetBondBetweenAtoms(0,1)))\n",
    "    # construct node feature matrix X of shape (n_nodes, n_node_features)\n",
    "    X = np.zeros((n_nodes, n_node_features))\n",
    "    for atom in mol.GetAtoms():\n",
    "        X[atom.GetIdx(), :] = get_atom_features(atom)\n",
    "\n",
    "    X = torch.tensor(X, dtype = torch.float)\n",
    "\n",
    "    # construct edge index array E of shape (2, n_edges)\n",
    "    (rows, cols) = np.nonzero(GetAdjacencyMatrix(mol))\n",
    "    torch_rows = torch.from_numpy(rows.astype(np.int64)).to(torch.long)\n",
    "    torch_cols = torch.from_numpy(cols.astype(np.int64)).to(torch.long)\n",
    "    E = torch.stack([torch_rows, torch_cols], dim = 0)\n",
    "\n",
    "    # construct edge feature array EF of shape (n_edges, n_edge_features)\n",
    "    EF = np.zeros((n_edges, n_edge_features))\n",
    "\n",
    "    for (k, (i,j)) in enumerate(zip(rows, cols)):\n",
    "\n",
    "        EF[k] = get_bond_features(mol.GetBondBetweenAtoms(int(i),int(j)))\n",
    "\n",
    "    EF = torch.tensor(EF, dtype = torch.float)\n",
    "\n",
    "    # construct label tensor\n",
    "    y_tensor = torch.tensor(np.array([y_val]), dtype = torch.float)\n",
    "\n",
    "    # construct Pytorch Geometric data object and append to data list\n",
    "    #data_list.append(Data(x = X, edge_index = E, edge_attr = EF, y = y_tensor))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
